{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Super Resolution - Kelvins PROBA V - Deep Learning Approach\n",
    "\n",
    "Each scene contains multiples images in Low Resolution and the goal is to obtain one single High Resolution image, therefore we can consider our task as a multiple image super resolution. But, each scene has a number non-fixed of images, so it can be difficult to realise a multi image super resolution using all images of our scene. (we know thought each scene has at least 9 images in Low Resolution). That is why, two main solutions are possible :\n",
    "- solve the task using single image super resolution, then average the prediction on the most clear images of a scene.\n",
    "- concatenate k fixed image. To select this k images, we can take the top k clearer images of a scene(and duplicate few of them if there are less than k image for a specific scene) -> multi image super resolution\n",
    "\n",
    "So, model used for single image super resolution will be implemented and tested on this dataset. Two architecture will be tested :\n",
    "- SRCNN (Super Resolution Convolutionnal Neural Network) [[1]](https://arxiv.org/pdf/1501.00092.pdf)\n",
    "- FSRCNN (Fast Super Resolution Convolutionnal Neural Network) [[2]](https://arxiv.org/pdf/1608.00367.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SRCNN\n",
    "\n",
    "The goal of this method is to learn an end to end mapping between low resolutions images and high resolutions images. The mapping take as input the low resolution image which is upscaled using a bicubic interpolation. The inputs feed then a deep neural network (CNN) which outputs the high resolution image. The intuition regarding the architecture of the CNN can be decomposed into 3 parts :\n",
    "- patch extraction and representation :  the process consist in extracting patches  from  the  low-resolution image and represents each patch as a high dimensional vector using convolutionnal filters. If we use a convolutionnal layer with N filters, then each patch will contain a high dimensional vector of size N.\n",
    "- non-linear mapping : this  operation  non linearly maps  each  high-dimensional  vector  onto  another high-dimensional  vector (but reduce the dimensionnality of the vector).  Each  mapped  vector  is the representation of a high-resolution patch. \n",
    "- reconstruction : this    operation    aggregates the above  high-resolution  patches  representations doing some kind of averaging in order to  generate  the  final  high-resolution  image.  This image is expected to be similar to the ground truth.\n",
    "\n",
    "These three operations which follow differents intuitions are represented by the same operation in the CNN : convolution layer. In fact, the patch extraction is a convolution layer with 64 filters with a kernel of 9 x 9. The non linear mapping is represented by a convolution layer with 32 filters with a kernel 1 x 1 and the reconstruction by a convolution layer with C filters (corresponding to the number of channels of the image in High Resolution) with a kernel of n x n. The bigger is n and the better are the results. (we define n=5)\n",
    "\n",
    "![image](image/srcnn.png)\n",
    "\n",
    "\n",
    "An improvement of this simple version exists and is called SRCNN-EX [[3]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.642.1999&rep=rep1&type=pdf) which increase the kernel size of the second convolution layer from 1 to 5 (non linear mapping part). In general, the authors observe that the bigger the kernel size  is or the number of filters is, the better is the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FSRCNN\n",
    "\n",
    "The goal of FSRCNN was to accelerate the SRCNN architecture, and propose a CNN structure faster and better at Super Resolution task. The SRCNN structure is mainly different in three points :\n",
    "- use smaller filter sizes but more mapping layers. \n",
    "- shrink the input feature dimension before the mapping using convolution layer with kernel size of 1 x 1. That permits to reduce the computing time. For instance, if the input feature map has a depth of d, then the convolution layer permetting to shrink these features need to have s filters with s << d. After the mapping is done, the mapping is expanded. The goal is to increase the dimension of the feature vector in order to get better reconstruction for the generation of the High Resolution. To do that, we use the same approach than the shrinking, i.e using convolutional layer but instead of having s filters (s << d) we have d filters.\n",
    "- introduction of  a  deconvolution  layer (called also transpose convolution)  at  the  end  of  the  network. The deconvolution will upsample and aggregate the previous features using a set of deconvolution filters. Therefore, the mapping is learned directly from the original Low Resolution image without bicubic interpolation, to the High Resolution one.\n",
    "\n",
    "The extraction of features part is composed of a convolution layer with 56 filters and a kernel size of 5.\n",
    "The shrinking part is composed of a convolution layer with 12 filters and a kernel size of 1.\n",
    "The mapping part is composed of 4 convolution layers with 12 filters and a kernel size of 3. The expanding part is composed of a convolution layers with 56 filters and a kernel size of 1. And finally, the deconvolution part is composed of a deconvolution layers with C filters (corresponding to the number of channel desired) and a kernel size of 9.\n",
    "\n",
    "![image](image/fsrcnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation :\n",
    "\n",
    "What was try :\n",
    "\n",
    "- train the different model using simple loss function such as MSE (Mean Squarred Error) between only the HR image and SR image (predicted image). The result was bad for all of them, (more than 1.0 so worse than the bicubic interpolation baseline). The reason was that, the simple mse loss function take in account only the pixel for the prediction image and the high resolution image, but the clearer image (SM) is also important. The hypothesis is that knowing which pixel is important to reconstruct and which one is not (for instance the reconstruction of clouds is meaningless in our task) help to improve the precision of the model. Therefore, instead of using MSE function, using a function based on [cPSNR](https://kelvins.esa.int/proba-v-super-resolution/scoring/) is more appropriate. So the custom loss is such as Loss =  $\\frac{N}{cPSNR}$. N = 46.5 represents the mean value of the norm provided in the csv file.\n",
    "\n",
    "- two approachs of predictions were tested : \n",
    "    - multi image super resolution : for training or testing with this method, I take the top k low resolution images for each scene where the number of clear pixel is maximum (based on the QM image). The model will then predict an \"High Resolution\" based on the k low resolution images.\n",
    "    - average single image super resolution : for training the model on single image as input, I take all images for each scene which have the maximum number of clear pixel (Quality Map image). Then the validation/testing is computing predicting on single low resolution image which have the maximum number of clearance pixel in a scene. If there are multiple low resolution images which have the same maximum number of clear pixel for a specific scene, I average the prediction.\n",
    "    \n",
    "    \n",
    "- inputs images : Two images are available as input, the low resolution (LR) image and the clearance image (MQ, called map quality). Each of these image are in gray scale, so they have only one channel. Three approachs were tested :\n",
    "    - use only the LR image. So the input does not know anything about clear pixel but it can deduce it because the output est the High Resolution image and its clearance image (SM)\n",
    "    - combine the LR image with the clearance image. (multiply them) So, each unclear pixel (clouds for instance) will have a value of 0. That means the image will loss the information about unclear pixel and the intuition is that : it could help the model to not reconstruct meaningless information as clouds.\n",
    "    - use the LR image and the clearance image. I concatenate these two images so instead of having an image with one channel, I will have two channel (one for the low resolution, one for the clearance). The intuition is the same as before, excepted that it will choose if it want to loose it or not. But, there will be twice the number of convolutions for the first convolutionnal layers compare to the previous proposition.\n",
    "\n",
    "\n",
    "\n",
    "- for SRCNN : add convolution block from VGG16 in order to see if it can increase the results of SRCNN. (as features extractors)\n",
    "\n",
    "- for FSRCNN : increase the kernel size to see if it performs better.\n",
    "\n",
    "# Results (validation):\n",
    "\n",
    "- use the custom loss based on cPSNR \\& cMSE performs far better than simple MSE.\n",
    "- the multi image super resolution approachs seems to have better results than averaging single image super resolution (based on SRCNN results). The hypothesis is that : when the multi image super resolution is used, it is possible to have different images with different clear part in each image. So the model may be able to combine the clear area in the different images in order to improve the reconstruction. \n",
    "- it seems there is not so much differences between using only LR image and concatenante LR and clearance image. But combining LR and the clearance image seems to bring a loss of  meaningfull informations for the reconstruction.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "| Model          | Prediction           | Inputs            | Results validation    | Results Test |\n",
    "| :------------- | :-------------------:|:----------------: |-----------: | ----------:|\n",
    "|  SRCNN         | Average single image | Only LR           | <strong>0.99625 </strong>     |  |\n",
    "|  SRCNN         | Average single image | Combine LR \\& QM  | 0.99896     |  |\n",
    "|  SRCNN         | Average single image | Concat LR \\& QM   | 0.99745     |  |\n",
    "|  SRCNN         | Multi image          | Only LR           | <strong>0.98961  </strong>   |  |\n",
    "|  SRCNN         | Multi image          | Combine LR \\& QM  | 0.99229     |  |\n",
    "|  SRCNN         | Multi image          | Concat LR \\& QM   | 0.99143     |  |\n",
    "|  SRCNNex       | Multi image          | Only LR           | 0.99000     |  |\n",
    "|  SRCNNex       | Multi image          | Concat LR \\& QM   | <strong>0.98896    </strong>  |  |\n",
    "|  FSRCNN        | Multi image          | Only LR           | 0.99134     | |\n",
    "|  FSRCNN        | Multi image          | Concat  LR \\& QM  | 0.99201     |   |\n",
    "|  SRCNN - VGG16 | Multi image          | Concat  LR \\& QM  | <strong>0.98886   </strong>   | <strong>0.984 </strong> |\n",
    "|  SRCNN - VGG16 | Multi image          | Only LR           | <strong>0.98996   </strong>   |   |\n",
    "\n",
    "\n",
    "- However we can see few artefacts generated by the model. The shape seems a bit better with the model than the bicubic model. There are improvement to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions_baseline_opencv import load_image2D\n",
    "from matplotlib import pyplot as plt\n",
    "opencvHR = load_image2D(\"image/imgset0594.png\")\n",
    "hr = load_image2D(\"image/HR_imgset0594.png\")\n",
    "preds_DL = load_image2D(\"image/SRVGG16_imgset0594.png\")\n",
    "\n",
    "print(preds_DL)\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "min_ = None\n",
    "max_ = None\n",
    "ax1 = fig.add_subplot(131); ax1.imshow(opencvHR, vmin=min_, vmax=max_); ax1.axis('on'); ax1.title.set_text('Bicubic High Resolution')\n",
    "ax2 = fig.add_subplot(132); ax2.imshow(preds_DL, vmin=min_, vmax=max_); ax2.axis('on'); ax2.title.set_text('SRVGG16 High Resolution')\n",
    "ax3 = fig.add_subplot(133); ax3.imshow(hr, vmin=min_, vmax=max_); ax3.axis('on'); ax3.title.set_text('High Resolution')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import cv2\n",
    "import numba\n",
    "from numba import prange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-315215131c20>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrescale\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mskimage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "### Package\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "from skimage import io \n",
    "from skimage.transform import rescale\n",
    "import skimage\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Convolution2D, Conv2DTranspose\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "import random\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from keras.optimizers import SGD, Adam, Nadam\n",
    "random.seed(20)\n",
    "import time\n",
    "import gc\n",
    "import numba\n",
    "from numba import prange #parallise loop\n",
    "from generator import batch_generator_SRCNN, batch_generator_SRCNN_validation, load_image2D\n",
    "from functions import load_data, preprocess_data, cPSNR_callback\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "from keras.losses import binary_crossentropy\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "from model import FSRCNN, SRCNN, SRCNNex, SRCNNv2, SRVGG16, FSRCNNv2, SRCNNv3, SRResnet\n",
    "from model import custom_loss, PSNR, MSE\n",
    "from baseline_cnn import compute_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train data\n",
    "datas = load_data(\"data/train.txt\")\n",
    "\n",
    "data_train, data_val = train_test_split(datas, test_size=0.1, shuffle=True, \n",
    "                                        random_state=42)\n",
    "\n",
    "# Parameters\n",
    "multi_output=True # let it True, allow to use custom loss function (the output is the clearance image and HR image)\n",
    "scale = 3 #upscale parameters\n",
    "resize=False # use bicubic interpolation or not  to upscale before feeding CNN\n",
    "with_clearance= True  # use clear image as input\n",
    "type_clearance= \"concat\" # if yes, concat or \"sum\" (combine)\n",
    "version=4 # version = 1 if we use the average single image approach, version = 4 if we use multi image approach\n",
    "k=9\n",
    "version_val = 1 if version !=4 else 4\n",
    "\n",
    "batch_size=32\n",
    "c = 1 if version != 4 else  k\n",
    "channel = c*1 if with_clearance == False or type_clearance==\"sum\" else c*2 # number of channels\n",
    "dim = 128 if resize else 128*scale\n",
    "\n",
    "# preprocess data\n",
    "\n",
    "train = preprocess_data(data_train, istrain=True,version=version, k = k)\n",
    "val  = preprocess_data(data_val, istrain=True, version=version_val, k = k)\n",
    "\n",
    "# parameters model\n",
    "opt = Nadam(0.001) #optimizer\n",
    "\n",
    "name_mod = \"FSRCNN_v4_noclearance_k9_concat.hdf5\" # name of the model to save at each iteration\n",
    "checkpoint = ModelCheckpoint(name_mod, verbose=2, monitor='val_cPSNR_'+str(version_val ), save_best_only=True, \n",
    "                             save_weights_only=False, mode='min')\n",
    "\n",
    "lr_decay = ReduceLROnPlateau(monitor='val_cPSNR_'+str(version_val), factor=0.5, patience=5,  verbose=1, mode='min', \n",
    "                             epsilon=0.0001, cooldown=0, min_lr=0.000000001) \n",
    "\n",
    "model = FSRCNN((dim, dim, channel), depth_multiplier=1, multi_output=multi_output, scale=scale)\n",
    "model.summary()\n",
    "if multi_output:\n",
    "    model.compile(loss=custom_loss, optimizer=opt)\n",
    "else:\n",
    "    model.compile(loss=MSE, optimizer=opt)\n",
    "    \n",
    "# create batch\n",
    "gen_train = batch_generator_SRCNN(train, batch_size=batch_size, with_clearance=with_clearance, type_clearance=type_clearance,\n",
    "                                    version=version, shuffle=True, scale=scale, data_aug=False, resize=resize, multi_output=multi_output)\n",
    "metrics = cPSNR_callback(val, with_clearance=with_clearance, type_clearance=type_clearance, version=version_val, \n",
    "                         scale=scale, resize=resize, name=\"val_\", multi_output=multi_output)\n",
    "\n",
    "steps_train=compute_steps(train, batch_size,  version=version)\n",
    "\n",
    "# train model\n",
    "model.fit_generator(gen_train, steps_per_epoch=steps_train, epochs=150, verbose=1, \n",
    "                    callbacks=[metrics, lr_decay, checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prediction import predict_results, get_validation_results\n",
    "\n",
    "# load train data\n",
    "datas = load_data(\"data/train.txt\")\n",
    "\n",
    "data_train, data_val = train_test_split(datas, test_size=0.1, shuffle=True, \n",
    "                                        random_state=42)\n",
    "data_test = load_data(\"data/test.txt\")\n",
    "# Parameters\n",
    "multi_output=True # let it True, allow to use custom loss function (the output is the clearance image and HR image)\n",
    "scale = 3 #upscale parameters\n",
    "resize=False # use bicubic interpolation or not  to upscale before feeding CNN\n",
    "with_clearance= True  # use clear image as input\n",
    "type_clearance= \"concat\" # if yes, concat or \"sum\" (combine)\n",
    "version=4 # version = 1 if we use the average single image approach, version = 4 if we use multi image approach\n",
    "k=9\n",
    "version_val = 1 if version !=4 else 4\n",
    "\n",
    "# preprocess data\n",
    "train = preprocess_data(data_train, istrain=True,version=version,k=k)\n",
    "val  = preprocess_data(data_val, istrain=True, version=version_val,k=k)\n",
    "\n",
    "# name model to load\n",
    "name_model =\"SRVGG16_v4_withclearance_concat_multi_k9\"#\"SRCNNv1\"#\"FSRCNNv1_withclearance_sum\"\n",
    "formats = \".hdf5\"\n",
    "model = load_model(name_model+formats)\n",
    "\n",
    "# validation\n",
    "evaluate=True\n",
    "if evaluate:\n",
    "    get_validation_results(model, train, with_clearance=with_clearance,  multi_output=multi_output,\n",
    "                       type_clearance=type_clearance, scale=scale, resize=resize, version=version)\n",
    "    get_validation_results(model, val4, with_clearance=with_clearance, multi_output=multi_output,\n",
    "                       type_clearance=type_clearance, scale=scale, resize=resize, version=version)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "directory = \"results_\" + name_model\n",
    "predict_results(model, test, directory, with_clearance=with_clearance, multi_output=multi_output,\n",
    "                       type_clearance=type_clearance, scale=scale, resize=resize, version=version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvements to do :\n",
    "- Use more layers in order to see if the results can be better\n",
    "- Use GAN (Generative Adversarial Neural Network) model. They seem to perform better than regular CNN.\n",
    "- Pretrained the model on satellite images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
